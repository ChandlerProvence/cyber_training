{"cells":[{"cell_type":"markdown","id":"ca388dac","metadata":{"id":"ca388dac"},"source":["\n","\n","#  <span style=\"color:#0b186c;\">Introduction to Machine Learning</span>\n","\n","---\n","\n","\n","\n","“Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.” – IBM 2020\n","\n","<br></br>\n","\n","## <span style=\"color:#0b186c;\">Table of Contents:</span>\n","* [Supervised vs Unsupervised Learning](#first-bullet)\n","* [Dataset Information](#second-bullet)\n","* [Supervised Learning](#third-bullet)\n","* [Unsupervised Learning](#fourth-bullet)\n","* [Conclusion](#fifth-bullet)"]},{"cell_type":"markdown","id":"868f69fb","metadata":{"id":"868f69fb"},"source":["#  <span style=\"color:#0b186c;\">Supervised vs Unsupervised Learning</span><a class=\"anchor\" id=\"first-bullet\"></a>\n","\n","---\n","## <span style=\"color:#0b186c;\">Reuired Imports:</span>\n","\n","<div class=\"alert alert-warning\">\n","\n","<b>Note:</b> If you have not previously installed these `packages`, you can use the cell below to perform the required `pip` installs.\n","\n","</div>"]},{"cell_type":"code","execution_count":null,"id":"52fcc492","metadata":{"id":"52fcc492"},"outputs":[],"source":["# In case you still need to perform some pip installs:\n","! pip install --user pandas -q\n","! pip install --user numpy -q\n","! pip install --user scikit-learn -q"]},{"cell_type":"code","execution_count":null,"id":"a351c4f0","metadata":{"id":"a351c4f0"},"outputs":[],"source":["# Dataframe and array libraries\n","import pandas as pd\n","import numpy as np\n","\n","# Libraries for visualizing data\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Retrieves the dataset from Scikit-learn\n","from sklearn.datasets import load_iris\n","\n","# Required for performing standardization\n","from sklearn.preprocessing import StandardScaler\n","\n","# Required for training and validating a model\n","from sklearn.model_selection import train_test_split\n","\n","# Required for instantiating and running a Decision Tree model\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import tree\n","\n","# Classification metrics and confusion matrix\n","from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, ConfusionMatrixDisplay\n","\n","# Required for instantiating and running a KMeans clustering model\n","from sklearn.cluster import KMeans\n","\n","# Filters out warning messages\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","id":"b65d3f50","metadata":{"id":"b65d3f50"},"source":["#  <span style=\"color:#0b186c;\">Dataset Information</span><a class=\"anchor\" id=\"second-bullet\"></a>\n","\n","---\n","\n","We will be using a dataset containing 3 species in the Iris genus, namely, Iris Setosa, Iris Versicolor and Iris Virginica found in the Gaspé Peninsula. For the purposes of an integral study, the collected Iris samples were, \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus.\" The dataset contains 150 rows of data, 50 rows of data for each species of Iris flower. The column names represent the feature of the flower that was studied and recorded.\n","\n","Our target dataset can be found in the Scikit-learn library, so we will be importing it directly from the library and storing it into a Pandas dataframe.\n","\n","https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"]},{"cell_type":"code","execution_count":null,"id":"5f81fe65","metadata":{"id":"5f81fe65"},"outputs":[],"source":["# Import the iris dataset\n","iris = load_iris(as_frame=True)\n","\n","# Place the dataset into a dataframe\n","df = iris.frame \n","\n","# View the first 5 records in the dataset\n","df.head()"]},{"cell_type":"markdown","id":"29e2960f","metadata":{"id":"29e2960f"},"source":["<div class=\"alert alert-info\">\n","   \n","We can use the `.info()` method for our dataframe to view a concise summary of the information contained within. This includes the number of observations, columns and data types, and any missing values.\n","    \n"," </div>"]},{"cell_type":"code","execution_count":null,"id":"b653241e","metadata":{"id":"b653241e"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","id":"51d30e12","metadata":{"id":"51d30e12"},"source":["<div class=\"alert alert-info\">\n","   \n","For numerical features in the dataframe, we can use the `.describe()` method to view relevant statistical information about each of the features. Understanding these values can assist in identifying the presence of outliers.\n","    \n"," </div>"]},{"cell_type":"code","execution_count":null,"id":"c55b92bc","metadata":{"id":"c55b92bc"},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","id":"dd8171f2","metadata":{"id":"dd8171f2"},"source":["<div class=\"alert alert-info\">\n","   \n","Additionally, we can use a `.pairplot()` from the `seaborn` library to visualize a scatter matrix of the independent variables. We can color code the plotted points based on the `target` feature to identify any discernable patterns in the measurement values.\n","    \n"," </div>"]},{"cell_type":"code","execution_count":null,"id":"d6cef55d","metadata":{"id":"d6cef55d"},"outputs":[],"source":["# Set the figure size\n","sns.set(rc={'figure.figsize':(12,8),'ytick.labelsize':(12)})\n","\n","# Create a pairplot\n","sns.pairplot(df, hue = \"target\", palette = \"Set2\")"]},{"cell_type":"markdown","id":"d8f077d7","metadata":{"id":"d8f077d7"},"source":["<div class=\"alert alert-info\">\n","   \n","Our dataset contains an equal number of observations for each of the Iris flowers. We can visualize the target variable distributions with a pie chart:\n","    \n","</div>"]},{"cell_type":"code","execution_count":null,"id":"9e68de34","metadata":{"id":"9e68de34"},"outputs":[],"source":["# Create a pie chart for the target variable\n","df.target.value_counts().plot(kind='pie', figsize=(8, 8), fontsize=10, autopct='%1.0f%%')\n","plt.title(\"Target Variable Distribution\", fontsize = 20)\n","plt.show()"]},{"cell_type":"markdown","id":"4c1c8bcc","metadata":{"id":"4c1c8bcc"},"source":["<div class=\"alert alert-info\">\n","   \n","Lastly, we can use the `.corr()` method on our dataframe to identify linear relationships between the independent variables and the dependent variable. This also helps identify collinearity that may exist amongst the independent variables as well. The correlation matrix can be enhanced by using a `.heatmap()` from the `seaborn` library that scales the specified hue based on the severity of the linear relationship.\n","    \n","</div>"]},{"cell_type":"code","execution_count":null,"id":"eb90c5b2","metadata":{"id":"eb90c5b2"},"outputs":[],"source":["# Set the figure size\n","sns.set(rc={'figure.figsize':(12,8),'ytick.labelsize':(12)})\n","\n","# Use the corr method to create the correlation matrix\n","correlation_matrix = df.corr().round(2)\n","\n","# Create a heatmap based on the severity of the linear relationship\n","sns.heatmap(data = correlation_matrix, annot = True, cmap = \"Blues\")\n","plt.title(\"Variable Correlation Heatmap\\n\", fontsize = 20)\n","plt.show()"]},{"cell_type":"markdown","id":"7cbfe0a7","metadata":{"id":"7cbfe0a7"},"source":["#  <span style=\"color:#0b186c;\">Supervised Learning</span><a class=\"anchor\" id=\"third-bullet\"></a>\n","\n","---\n","\n","In Supervised Learning, the algorithms are provided with a combination of independent variables (X) and a labeled dependent variable (y). The algorithm learns how to map to the desired output based on the input-output pairs in the training process. Supervised Learning can be dissected into two subcategories, Regression and Classification.\n","\n","- Regression models predict a continous, numerical output value.\n","- Classification models predict a discrete, categorical output value."]},{"cell_type":"markdown","id":"3a477276","metadata":{"id":"3a477276"},"source":["## <span style=\"color:#0b186c;\">Classification of Iris Flowers</span>\n","\n","Since our target variable represents discrete, categorical representations of the different genus of Iris flowers, we will be building a classification model. One of the simplest forms of a classification model is the `Decision Tree`. First, we have to \n","partition our dataset into a training and test set.\n","\n","<div class=\"alert alert-info\">\n","    \n","&nbsp;**Note:** It is imperative that the subsets are representative of the whole &nbsp;dataset. \n","The best way to accomplish this is using the built-in function, &nbsp;`train_test_split()`.\n","\n","</div>\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"]},{"cell_type":"code","execution_count":null,"id":"d45e1dd0","metadata":{"id":"d45e1dd0"},"outputs":[],"source":["# Split the independent (X) and dependent (y) variables\n","X = df.iloc[:, :-1]\n","y = df.iloc[:, -1].values\n","\n","# Split the data into an 80/20 \n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n","\n","# Output the shape of the training set\n","X_train.shape"]},{"cell_type":"markdown","id":"4c70ab93","metadata":{"id":"4c70ab93"},"source":["Using **standardization**, we can change the form of our features into a normal distribution, so that it easier to correctly represent the feature weights in the modeling process.\n","\n","The `StandardScaler()` from `scikit-learn` standardizes independently on each feature by setting the mean to 0 and the standard deviation to 1 to accomplish the scaling appropriately. First, the scaler has to be fit on the training data to learn the relevant statistics. Using the `.fit_transform()` method, we can fit and simultaneously transform the training data in a single line of code. The test data is then transformed using the `.transform()` method.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"]},{"cell_type":"code","execution_count":null,"id":"5c9e5b34","metadata":{"id":"5c9e5b34"},"outputs":[],"source":["# Instantiate the standard scaler\n","sc = StandardScaler()\n","\n","# Fit and transform the scaler on the training set\n","X_train = sc.fit_transform(X_train) \n","\n","# Transform the fit scaler on the test set\n","X_test = sc.transform(X_test) "]},{"cell_type":"markdown","id":"21cf1908","metadata":{"id":"21cf1908"},"source":["Decision Trees are one of the most popular types of Classification algorithms due to their flexibility on handling missing values and different data types in the input variables. The Decision Tree creates a flowchart tree structure, where each internal node denotes a test on an independent variable. For each test, branches are created based on the outcome. This process continues to a terminal node, which holds the decided class label from the dependent variable. The model can be loaded directly from `scikit-learn`.\n","\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"]},{"cell_type":"code","execution_count":null,"id":"9eafd5d1","metadata":{"id":"9eafd5d1"},"outputs":[],"source":["# Instantiate the classifier\n","classifier = DecisionTreeClassifier()\n","\n","# Fit the model on the training data\n","classifier.fit(X_train, y_train)\n","\n","# Plot the tree structure\n","fig = plt.figure(figsize=(25,20))\n","_ = tree.plot_tree(classifier, \n","                   max_depth = 2,\n","                   feature_names= X.columns,\n","                   class_names = True,\n","                   filled = True)"]},{"cell_type":"markdown","id":"6545cafa","metadata":{"id":"6545cafa"},"source":["Once the model has been trained, we can use the test data to validate our model and identify how well our model's **generalizations** are in comparison to the real events in `y_test`. This is where we can identify potential underfitting or overfitting of our model."]},{"cell_type":"code","execution_count":null,"id":"78038085","metadata":{"id":"78038085"},"outputs":[],"source":["# Make predictions based on the X values in the test set\n","y_pred = classifier.predict(X_test)\n","\n","# Calculate the accuracy score of the test set\n","score = round((accuracy_score(y_test, y_pred) * 100), 2)\n","\n","# changing the rc parameters to adjust the size\n","plt.rcParams['figure.figsize'] = [10, 10]\n","\n","#Plot the confusion Matrix for the predictions\n","fig = plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.Blues)\n","fig.ax_.set_title(\"Confusion Matrix\")\n","plt.grid(False)\n","plt.show()\n","\n","# Print the accuracy score on the validation data\n","print(f\"Accuracy = {score}%\")"]},{"cell_type":"markdown","id":"7ea738a9","metadata":{"id":"7ea738a9"},"source":["#  <span style=\"color:#0b186c;\">Unsupervised Learning</span><a class=\"anchor\" id=\"fourth-bullet\"></a>\n","\n","---\n","\n","In Unsupervised Learning, the algorithms are not provided with an expected output in the form of a dependent variable. The algorithm extrapolates patterns from the input variables and draws its own conclusions about the unlabeled data. Unsupervised Learning can primarily be grouped into two subcategories, Clustering and Dimensionality Reduction.\n","\n","- Clustering models group data points based on similarity and separate groups by dissimilarity.\n","- Dimensionality Reduction models transform data from the original dimensional space into a smaller dimension, while still capturing meaningful variance in the data."]},{"cell_type":"markdown","id":"a734838b","metadata":{"id":"a734838b"},"source":["## <span style=\"color:#0b186c;\">Clustering of Unlabeled Flowers</span>\n","\n","The independent variables from the iris dataset can be isolated without the target label to perform clustering. This provides a good opportunity to compare the outcomes of clustering with the known labels in the dataset. One of the simplest forms of a clustering model is the `KMeans`. First, let's review the independent variables stored in `X`:"]},{"cell_type":"code","execution_count":null,"id":"4c8b85be","metadata":{"id":"4c8b85be"},"outputs":[],"source":["# Review the input variables without a target label\n","X"]},{"cell_type":"markdown","id":"ecab15da","metadata":{"id":"ecab15da"},"source":["Clustering algorithms work by defining clusters, or groups of data points, such that the total intra-cluster variation is minimized. The KMeans algorithm accomplishes this by decreasing the within-cluster sum of squares, or the deviations from each observation and the cluster centroid. The centroids are what determines the total number of cluster centers present in the data. The Naive method for determining the optimal number of clusters is by plotting the inertia, which represents the within-cluster sum of squares, against the number of clusters and identifying the `elbow curve`.\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"]},{"cell_type":"code","execution_count":null,"id":"8d11560f","metadata":{"id":"8d11560f"},"outputs":[],"source":["# Instantiate the KMeans algorithm, fit on 1 to 10 clusters\n","km = [KMeans(n_clusters=i).fit(X) for i in range(1, 11)]\n","\n","# Calculate the within-cluster sum of squares for each number of clusters\n","scores = [km[i].score(X) for i in range(len(km))]\n","\n","# Plot the number of clusters against their respective within-cluster sum of squares\n","fig, ax = plt.subplots(figsize=(10,6))\n","ax.plot(range(1, 11), scores)\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('Score')\n","plt.title('Elbow Curve')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"94242a6f","metadata":{"id":"94242a6f"},"outputs":[],"source":["# Instantiate the clustering algorithm with optimal number of clusters\n","km = KMeans(n_clusters=3)\n","\n","# Fit on the input data\n","km.fit(X)\n","\n","# Predict the labels (assigned cluster)\n","km.predict(X)\n","\n","# View the output labels\n","print(km.labels_)"]},{"cell_type":"markdown","id":"117b09c9","metadata":{"id":"117b09c9"},"source":["<img src = \".\\Media\\iris_pairplot.PNG\" align = \"right\" width = \"52%\">\n","\n","Our original data had 3 different types of Iris genus, each with distinctive features. We can compare the outcome of our clustering against the `.pairplot()` on the original data. As you can see, the Setosa genus is starkly different from the other 2 types of flowers. There is a less discernable line between the Virginica and Versicolor species, which is reflected in the overlap of data points.\n","\n","In the `elbow curve` above, we saw that the greatest variance was captured at the 2nd cluster. After the 3rd cluster, there was very little difference to the captured variance. This is highlights the difference between working with labeled data in Supervised Learning vs unlabeled data in Unsupervised Learning."]},{"cell_type":"code","execution_count":null,"id":"6e9d87b4","metadata":{"id":"6e9d87b4"},"outputs":[],"source":["# Set the figure size\n","sns.set(rc={'figure.figsize':(12,8),'ytick.labelsize':(12)})\n","\n","# Add the cluster labels to the dataframe\n","X['clusters'] = km.labels_\n","\n","# Create a pairplot with the color-coded clusters\n","sns.pairplot(X, hue = \"clusters\", palette = \"Set2\")"]},{"cell_type":"code","execution_count":null,"id":"acc0d980","metadata":{"id":"acc0d980"},"outputs":[],"source":["# Create a pie chart for the cluster variable\n","X.clusters.value_counts().plot(kind='pie', figsize=(8, 8), fontsize=10, autopct='%1.0f%%')\n","plt.title(\"Target Cluster Distribution\", fontsize = 20)\n","plt.show()"]},{"cell_type":"markdown","id":"3438c987","metadata":{"id":"3438c987"},"source":["#  <span style=\"color:#0b186c;\">Conclusion</span><a class=\"anchor\" id=\"fifth-bullet\"></a>\n","\n","---\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true}},"nbformat":4,"nbformat_minor":5}