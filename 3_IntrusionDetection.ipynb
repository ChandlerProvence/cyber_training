{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f82b1e60",
      "metadata": {
        "id": "f82b1e60"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Introduction to Intrusion Detection Systems (IDS)</span>\n",
        "\n",
        "---\n",
        "\n",
        "An Intrusion Detection System (IDS) is a system that **monitors** network traffic for suspicious activity and issues alerts when such activity is discovered. These systems can be applications that sit at specific network locations and can be tailored to search for known threats as well as abnormal behavior.\n",
        "\n",
        "<br></br>\n",
        "## <span style=\"color:#0b186c;\">Table of Contents:</span>\n",
        "* [Project Description](#first-bullet)\n",
        "* [Dataset Information](#second-bullet)\n",
        "* [Data Preprocessing](#third-bullet)\n",
        "* [Initial Model Development](#fourth-bullet)\n",
        "* [Dimensionality Reduction](#fifth-bullet)\n",
        "* [Conclusion](#sixth-bullet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec077397",
      "metadata": {
        "id": "ec077397"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Project Description</span><a class=\"anchor\" id=\"first-bullet\"></a>\n",
        "\n",
        "---\n",
        "\n",
        "Intrusion Detection Systems can use different methods to detect suspicious activities, which can be broadly divided into:\n",
        "\n",
        "- **Signature-based intrusion detection** – These systems compare the incoming traffic with a pre-existing database of known attack patterns known as signatures.\n",
        "\n",
        "- **Anomaly-based intrusion detection** – It uses statistics to form a baseline usage of the networks at different time intervals. They were introduced to detect unknown attacks. This system uses machine learning to create a model simulating regular activity and then compares new behaviour with the existing model.\n",
        "\n",
        "## <span style=\"color:#0b186c;\">Required Imports:</span>\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "\n",
        "<b>Note:</b> If you have not previously installed these `packages`, you can use the cell below to perform the required `pip` installs.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44b9c6d",
      "metadata": {
        "id": "e44b9c6d"
      },
      "outputs": [],
      "source": [
        "# In case you still need to perform some pip installs:\n",
        "! pip install --user pandas -q\n",
        "! pip install --user numpy -q\n",
        "! pip install --user scikit-learn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17bb9bcc",
      "metadata": {
        "id": "17bb9bcc"
      },
      "outputs": [],
      "source": [
        "# Dataframe and array libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# URL and System libraries for importing data\n",
        "import requests\n",
        "import io\n",
        "\n",
        "# Libraries for visualizing data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Required for performing standardization robust to outliers\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Required for performing encoding on categorical input variables\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Required for performing encoding categorical input variables into new columns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Required for instantiating and running a Decision Tree model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# Classification metrics and confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Required for performing dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Filters out warning messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f181a16",
      "metadata": {
        "id": "9f181a16"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Dataset Information</span><a class=\"anchor\" id=\"second-bullet\"></a>\n",
        "\n",
        "---\n",
        "\n",
        "Widely considered the *Hello World* of IDS Machine Learning, the KDD '99 dataset is used to create Supervised Machine Learning models capable of distinguishing normal network behavior from intrusions of varied attack types. The original dataset originated from the Third International Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99 the Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between bad connections, called intrusions or attacks, and good normal connections. This database contains a standard set of data to be audited, which includes a wide variety of intrusions simulated in a military network environment.\n",
        "\n",
        "NSL-KDD is a dataset suggested to solve some of the inherent problems of the KDD'99 data set, which included a large amount of redundant and duplicate records. The improvements made on the dataset reduce the potential for bias towards better detection rates on the more frequent records. The dataset is maintained by the Canandian Institute of Cybersecurity:\n",
        "\n",
        "https://www.unb.ca/cic/datasets/nsl.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6363e3",
      "metadata": {
        "id": "cc6363e3"
      },
      "outputs": [],
      "source": [
        "# Column names for the dataset\n",
        "cols = ['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot',\n",
        "        'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
        "        'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count',\n",
        "        'serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate',\n",
        "        'srv_diff_host_rate','dst_host_count','dst_host_srv_count','dst_host_same_srv_rate','dst_host_diff_srv_rate',\n",
        "        'dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "        'dst_host_rerror_rate','dst_host_srv_rerror_rate','target','difficulty']\n",
        "\n",
        "# Download the training data from github\n",
        "train_url = \"https://raw.githubusercontent.com/ChandlerProvence/cyber_training/main/KDDTrain.csv\"\n",
        "download = requests.get(train_url).content\n",
        "\n",
        "# Read in the training dataset\n",
        "train_df = pd.read_csv(io.StringIO(download.decode('utf-8')), names = cols)\n",
        "\n",
        "# Download the test data from github\n",
        "test_url = \"https://raw.githubusercontent.com/ChandlerProvence/cyber_training/main/KDDTest.csv\"\n",
        "download = requests.get(test_url).content\n",
        "\n",
        "# Read in the training dataset\n",
        "test_df = pd.read_csv(io.StringIO(download.decode('utf-8')), names = cols)\n",
        "\n",
        "# Output the top 5 records of the training set\n",
        "print(\"Training Set:\")\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a51c403",
      "metadata": {
        "id": "4a51c403"
      },
      "outputs": [],
      "source": [
        "# Output the top 5 records of the test set\n",
        "print(\"Test Set:\")\n",
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7de2d3",
      "metadata": {
        "scrolled": true,
        "id": "8c7de2d3"
      },
      "outputs": [],
      "source": [
        "# Drop the difficulty feature from both datasets\n",
        "train_df.drop(columns=['difficulty'], inplace=True)\n",
        "test_df.drop(columns=['difficulty'], inplace=True)\n",
        "\n",
        "# Output dataframe information for the training set\n",
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4489dfa5",
      "metadata": {
        "scrolled": true,
        "id": "4489dfa5"
      },
      "outputs": [],
      "source": [
        "# Output dataframe information for the test set\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e183b68f",
      "metadata": {
        "id": "e183b68f"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Data Preprocessing</span><a class=\"anchor\" id=\"third-bullet\"></a>\n",
        "\n",
        "---\n",
        "\n",
        "Data Preprocessing is the process of selecting and/or transforming existing features (columns) in the raw data to make the data compatible with the chosen machine learning model and improve the model’s predictive performance against the data. The appropriate preprocessing techniques for a particular model requires knowledge of how the model interprets the different features, as well as domain expertise pertaining to the data itself. Otherwise, if done improperly, the resulting predictions could be inaccurate, or the dependencies could be misinterpreted by the model. \n",
        "\n",
        "Common Data Preprocessing techniques include:\n",
        "\n",
        "- Imputation\n",
        "- Log Transformations\n",
        "- Encoding\n",
        "- Feature Selection\n",
        "- Scaling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92579948",
      "metadata": {
        "id": "92579948"
      },
      "source": [
        "## <span style=\"color:#0b186c;\">Numerical Features</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff4fed2",
      "metadata": {
        "id": "aff4fed2"
      },
      "outputs": [],
      "source": [
        "# Select the columns in the training set with numerical dtypes\n",
        "num_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a 4x5 figure to plot the first 20 box plots\n",
        "fig, axes = plt.subplots(4, 5)\n",
        "fig.suptitle('Training Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), num_cols[0:20]):\n",
        "    train_df[col].value_counts().plot(ax=ax, kind='box', figsize=(20, 20), fontsize=10)\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de9cdeb",
      "metadata": {
        "id": "8de9cdeb"
      },
      "outputs": [],
      "source": [
        "# Create a 3x6 figure to plot the remaining 18 box plots\n",
        "fig, axes = plt.subplots(3, 6)\n",
        "fig.suptitle('Training Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), num_cols[20:]):\n",
        "    train_df[col].value_counts().plot(ax=ax, kind='box', figsize=(20, 20), fontsize=10)\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6b5076",
      "metadata": {
        "id": "8a6b5076"
      },
      "outputs": [],
      "source": [
        "# Select the columns in the test set with numerical dtypes\n",
        "num_cols = test_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Create a 4x5 figure to plot the first 20 box plots\n",
        "fig, axes = plt.subplots(4, 5)\n",
        "fig.suptitle('Test Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), num_cols[0:20]):\n",
        "    test_df[col].value_counts().plot(ax=ax, kind='box', figsize=(20, 20), fontsize=10)\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d443809",
      "metadata": {
        "id": "9d443809"
      },
      "outputs": [],
      "source": [
        "# Create a 3x6 figure to plot the remaining 18 box plots\n",
        "fig, axes = plt.subplots(3, 6)\n",
        "fig.suptitle('Test Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), num_cols[20:]):\n",
        "    test_df[col].value_counts().plot(ax=ax, kind='box', figsize=(20, 20), fontsize=10)\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19040ed7",
      "metadata": {
        "id": "19040ed7"
      },
      "outputs": [],
      "source": [
        "# Instantiate the scaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the numerical columns in the training set\n",
        "train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
        "\n",
        "# Review the changes made to the training set\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d08c687",
      "metadata": {
        "id": "5d08c687"
      },
      "outputs": [],
      "source": [
        "# Transform the numerical test columns in the test set\n",
        "test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
        "\n",
        "# Review the changes made to the test set\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5f5627a",
      "metadata": {
        "id": "a5f5627a"
      },
      "source": [
        "## <span style=\"color:#0b186c;\">Categorical Features</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dbf1dc",
      "metadata": {
        "id": "54dbf1dc"
      },
      "outputs": [],
      "source": [
        "# Select the columns in the training set with dtype of object\n",
        "obj_cols = train_df.select_dtypes(include='object').columns\n",
        "\n",
        "# Create a 2x2 figure to plot pie charts of the categorical distributions\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "fig.suptitle('Training Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), obj_cols):\n",
        "    train_df[col].value_counts().plot(ax=ax, kind='pie', figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b715e95d",
      "metadata": {
        "id": "b715e95d"
      },
      "outputs": [],
      "source": [
        "# Select the columns in the test set with dtype of object\n",
        "obj_cols = test_df.select_dtypes(include='object').columns\n",
        "\n",
        "# Create a 2x2 figure to plot pie charts of the categorical distributions\n",
        "fig, axes = plt.subplots(2, 2)\n",
        "fig.suptitle('Test Set', fontsize=20)\n",
        "for ax, col in zip(axes.ravel(), obj_cols):\n",
        "    test_df[col].value_counts().plot(ax=ax, kind='pie', figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n",
        "    ax.set_title(str(col), fontsize = 12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb502af2",
      "metadata": {
        "id": "bb502af2"
      },
      "outputs": [],
      "source": [
        "# Instantiate the ordinal encoder\n",
        "oe = OrdinalEncoder(dtype=int, handle_unknown = 'use_encoded_value', unknown_value = 999)\n",
        "\n",
        "# Fit the encoder on the flag feature  \n",
        "oe.fit(train_df[['flag']])\n",
        "\n",
        "# View the identified categories in the fitted feature\n",
        "oe.categories_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "011b3e6b",
      "metadata": {
        "id": "011b3e6b"
      },
      "outputs": [],
      "source": [
        "# Transform the identified categories into numerical representations\n",
        "train_df['flag'] = oe.transform(train_df[['flag']])\n",
        "\n",
        "# Review the changes made to the flag feature in the training set\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d31128",
      "metadata": {
        "id": "24d31128"
      },
      "outputs": [],
      "source": [
        "# Transform the identified categories into numerical representations\n",
        "test_df['flag'] = oe.transform(test_df[['flag']])\n",
        "\n",
        "# Review the changes made to the flag feature in the test set\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95b7d1a",
      "metadata": {
        "id": "b95b7d1a"
      },
      "outputs": [],
      "source": [
        "# Fit on the service feature and transform in place\n",
        "train_df['service'] = oe.fit_transform(train_df[['service']])\n",
        "\n",
        "# Verify changes made to the training set\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fae66c86",
      "metadata": {
        "id": "fae66c86"
      },
      "outputs": [],
      "source": [
        "# Transform the categories identified in training for the test set\n",
        "test_df['service'] = oe.transform(test_df[['service']])\n",
        "\n",
        "# Verify changes made to the test set\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4253e324",
      "metadata": {
        "id": "4253e324"
      },
      "outputs": [],
      "source": [
        "# Instantiate the one hot encoder\n",
        "ohe = OneHotEncoder(sparse=False, dtype=int)\n",
        "\n",
        "# Fit the encoder on the appropriate column\n",
        "ohe.fit(train_df[['protocol_type']])\n",
        "\n",
        "# Select the categories identified by the encoder\n",
        "col = ohe.categories_\n",
        "\n",
        "# Run transform to one hot encode the column, adding the new columns to the dataframe\n",
        "train_df[col[0]] = ohe.transform(train_df[['protocol_type']])\n",
        "\n",
        "# Drop the original column, not needed anymore\n",
        "train_df.drop(columns=['protocol_type'], inplace=True)\n",
        "\n",
        "# Review the changes made to the training set\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3998acf8",
      "metadata": {
        "id": "3998acf8"
      },
      "outputs": [],
      "source": [
        "# Run transform to one hot encode the column, adding the new columns to the dataframe\n",
        "test_df[col[0]] = ohe.transform(test_df[['protocol_type']])\n",
        "\n",
        "# Drop the original column, not needed anymore\n",
        "test_df.drop(columns=['protocol_type'], inplace=True)\n",
        "\n",
        "# Review the changes made to the test set\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ff7b94",
      "metadata": {
        "id": "95ff7b94"
      },
      "outputs": [],
      "source": [
        "# Assign 0 for normal and 1 for intrusion in the target variable in both sets\n",
        "train_df.loc[(train_df.target == 'normal'), 'target'] = 0\n",
        "train_df.loc[(train_df.target != 0), 'target'] = 1\n",
        "\n",
        "test_df.loc[(test_df.target == 'normal'), 'target'] = 0\n",
        "test_df.loc[(test_df.target != 0), 'target'] = 1\n",
        "\n",
        "# Visualize the altered target variable distributions\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "c1 = ['#1f77b4', '#d62728']\n",
        "c2 = ['#d62728', '#1f77b4']\n",
        "train_df.target.value_counts().plot(ax=axes[0], kind='pie', colors = c1, \n",
        "                                    figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n",
        "test_df.target.value_counts().plot(ax=axes[1], kind='pie', colors = c2,\n",
        "                                   figsize=(15, 15), fontsize=10, autopct='%1.0f%%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c347f6",
      "metadata": {
        "id": "e0c347f6"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Initial Model Development</span><a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
        "\n",
        "---\n",
        "\n",
        "Since the dataset contains a labeled target variable, which we have scoped for binary classification of intrusions and normal network activity, we can leverage classification models for predictions on future data. As discussed previously, Decision Trees are one of the most popular types of classification algorithms due to their flexibility and overall performance. \n",
        "\n",
        "First, we will split the data into independent variables (X) and the dependent variable (y) for both the training and test sets. Since the data has already been partioned for us, we will not need to use the `train_test_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6839a894",
      "metadata": {
        "id": "6839a894"
      },
      "outputs": [],
      "source": [
        "# Separate X and y for the training set\n",
        "y_train = train_df.pop('target').astype('int')\n",
        "X_train = train_df\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98caf146",
      "metadata": {
        "id": "98caf146"
      },
      "outputs": [],
      "source": [
        "# Separate X and y for the test set\n",
        "y_test = test_df.pop('target').astype('int')\n",
        "X_test = test_df\n",
        "\n",
        "X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d185942a",
      "metadata": {
        "id": "d185942a"
      },
      "source": [
        "The Decision Tree model can be loaded directly from `scikit-learn`.\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7547a42",
      "metadata": {
        "id": "d7547a42"
      },
      "outputs": [],
      "source": [
        "# Instantiate the classifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model on the training data\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = tree.plot_tree(classifier, \n",
        "                   max_depth = 2,\n",
        "                   feature_names= X_train.columns,  \n",
        "                   class_names = ['Normal', 'Intrusion'],\n",
        "                   filled = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe3df821",
      "metadata": {
        "id": "fe3df821"
      },
      "outputs": [],
      "source": [
        "# Make predictions based on the X values in the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy score of the test set\n",
        "score = round((accuracy_score(y_test, y_pred) * 100), 2)\n",
        "\n",
        "# changing the rc parameters to adjust the size\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "#Plot the confusion Matrix for the predictions\n",
        "fig = plot_confusion_matrix(classifier, X_test, y_test, cmap = plt.cm.Blues)\n",
        "fig.ax_.set_title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f\"Accuracy = {score}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8b4e08",
      "metadata": {
        "id": "ab8b4e08"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Dimensionality Reduction</span><a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
        "\n",
        "---\n",
        "\n",
        "The number of input variables or features for a dataset is referred to as its dimensionality. More input features often make a predictive modeling task more challenging, generally due to the interpretability of feature importance and variance. Therefore, dimensionality reduction techniques are popular for handling data with hundreds, or even thousands, of features. One such technique, Principal Component Analysis (PCA), is a dimensionality reduction method that transforms the data to a new basis where the dimensions are non-redundant (low covariance) and have high variance. PCA is an Unsupervised Learning method and can be loaded directly from `scikit-learn`.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10952a93",
      "metadata": {
        "id": "10952a93"
      },
      "outputs": [],
      "source": [
        "# Review the training dataframe, it should not have the target label present\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b16a83",
      "metadata": {
        "id": "a8b16a83"
      },
      "outputs": [],
      "source": [
        "# Instantiate the PCA\n",
        "pca = PCA()\n",
        "\n",
        "# Fit pca on the training data\n",
        "pca.fit(train_df)\n",
        "\n",
        "# Plot the principal components against their inertia\n",
        "features = range(pca.n_components_)\n",
        "_ = plt.figure(figsize=(15, 5))\n",
        "_ = plt.bar(features, pca.explained_variance_)\n",
        "_ = plt.xlabel('Principal Component')\n",
        "_ = plt.ylabel('Variance')\n",
        "_ = plt.xticks(features)\n",
        "_ = plt.title(\"Importance of the Principal Components Based on Inertia\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "474534fe",
      "metadata": {
        "id": "474534fe"
      },
      "outputs": [],
      "source": [
        "# Instantiate pca with the optimal number of components\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# Since we reduced the number of features, we are going to use new column names\n",
        "pc_columns = ['pc_%i' % i for i in range(3)]\n",
        "\n",
        "# Transform the training data into reduced dimensions of 3 principal components\n",
        "pca_train = pd.DataFrame(pca.fit_transform(train_df), columns = pc_columns, index = train_df.index)\n",
        "\n",
        "# View the transformed dataframe\n",
        "pca_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9a939c",
      "metadata": {
        "id": "cb9a939c"
      },
      "outputs": [],
      "source": [
        "# Transform the test data into reduced dimensions of 3 principal components\n",
        "pca_test = pd.DataFrame(pca.transform(test_df), columns = pc_columns, index = test_df.index)\n",
        "\n",
        "# View the transformed dataframe\n",
        "pca_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72c2975a",
      "metadata": {
        "id": "72c2975a"
      },
      "outputs": [],
      "source": [
        "# Fit the model on the training data\n",
        "classifier.fit(pca_train, y_train)\n",
        "\n",
        "fig = plt.figure(figsize=(25,20))\n",
        "_ = tree.plot_tree(classifier, \n",
        "                   max_depth = 2,\n",
        "                   feature_names= pca_train.columns,  \n",
        "                   class_names = ['Normal', 'Intrusion'],\n",
        "                   filled = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad3792c",
      "metadata": {
        "id": "8ad3792c"
      },
      "outputs": [],
      "source": [
        "# Make predictions based on the X values in the test set\n",
        "y_pred = classifier.predict(pca_test)\n",
        "\n",
        "# Calculate the accuracy score of the test set\n",
        "score = round((accuracy_score(y_test, y_pred) * 100), 2)\n",
        "\n",
        "# changing the rc parameters to adjust the size\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "\n",
        "#Plot the confusion Matrix for the predictions\n",
        "fig = plot_confusion_matrix(classifier, pca_test, y_test, cmap = plt.cm.Blues)\n",
        "fig.ax_.set_title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f\"Accuracy = {score}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2819ab1b",
      "metadata": {
        "id": "2819ab1b"
      },
      "source": [
        "#  <span style=\"color:#0b186c;\">Conclusion</span><a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a5f5627a"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}